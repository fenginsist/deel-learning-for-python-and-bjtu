# åˆ©ç”¨torch.nnå®ç° softmax å›å½’åœ¨Fashion-MNISTæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œ
# å¹¶ä»lossï¼Œè®­ç»ƒé›†ä»¥åŠæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ç­‰å¤šä¸ªè§’åº¦å¯¹ç»“æœè¿›è¡Œåˆ†æ
# https://blog.csdn.net/qq_37534947/article/details/108179408


# æ³¨æ„çš„ç‚¹
# 1ã€lossè®°å¾— .sum()
# 2ã€å…ˆæ¢¯åº¦æ¸…é›¶

# é—®é¢˜ï¼š
# 1ã€å®šä¹‰æ¨¡å‹æ—¶ï¼šä¸ºå•¥è¿™é‡Œæ²¡æœ‰åƒ ä½œä¸š3ä¸€æ ·ï¼Œå…ˆè¿›è¡Œ çº¿æ€§åˆ¤åˆ« å†è¿›è¡Œ éçº¿æ€§å†³ç­–å‘¢ï¼Ÿåªçœ‹åˆ°äº†çº¿æ€§å‡½æ•°ï¼Œæ²¡æœ‰çœ‹åˆ°å†³ç­–å‡½æ•°ã€‚å›°æƒ‘
# 2ã€æ²¡æœ‰çœ‹åˆ° requires_grad=True çš„è®¾ç½®ï¼Œåº”è¯¥åœ¨å“ªé‡Œè®¾ç½®å‘¢ï¼Ÿ

import torch
import torch.nn as nn
from torch.nn import init as Init  # è°ƒç”¨ğ¢ğ§ğ¢ğ­. ğ§ğ¨ğ«ğ¦ğšğ¥æ¨¡å—é€šè¿‡æ­£æ€åˆ†å¸ƒå¯¹çº¿æ€§ å›å½’ä¸­çš„æƒé‡å’Œåå·®è¿›è¡Œåˆå§‹åŒ–
import numpy as np
import torchvision
import torchvision.transforms as transforms

# ä¸‹è½½çš„æ•°æ®
mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=False,
                                                transform=transforms.ToTensor())

# è¯»å–æ•°æ®
# batch_size æ¯æ¬¡æ‰¹é‡åŠ è½½ 256ä¸ªæ ·æœ¬
batch_size = 256
train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)

# æ„å»ºæ¨¡å‹
input_num = 784     # å›¾ç‰‡å¤§å°ä¸º28*28åƒç´ ï¼Œå› æ­¤è¾“å…¥ä¸º28*28=784
output_num = 10     # åˆ†ç±»æ•°ä¸º10ï¼Œå› æ­¤è¾“å‡ºä¸º10


# --------------------------------------æ³¨ï¼šè¿™é‡Œå’Œä¹‹å‰çš„logisticå®ç°å‡ ä¹ä¸€æ ·ï¼Œä¸»è¦æ˜¯å¯¹äºè¯¥æ¨¡å‹çš„è¾“å‡ºä»1å˜æˆ10.
class softmax_net(nn.Module):
    def __init__(self, input_num, output_num):
        super(softmax_net, self).__init__()
        self.linear = nn.Linear(input_num, output_num)  # å›¾ç‰‡å¤§å°ä¸º28*28åƒç´ ï¼Œå› æ­¤è¾“å…¥ä¸º28*28ï¼›åˆ†ç±»æ•°ä¸º10ï¼Œå› æ­¤è¾“å‡ºä¸º10
        self.sm = nn.Softmax(dim=1)                     # å¯¹çº¿æ€§å±‚çš„è¾“å‡ºå–Softmaxï¼Œè½¬æ¢ä¸ºæ¦‚ç‡

    def forward(self, x):  # x:Tensor:(256, 1, 28, 28)      # 256 æ˜¯æŒ‡ ä¸€ä¸ªbatch ä¸º 256ï¼Œä¸Šé¢æŒ‡å®šäº†ä¸º256ï¼Œæ‰€ä»¥è¿™é‡Œæ˜¯256
        y = self.linear(x.view(x.shape[0], -1))             # xçš„shapeï¼ˆåˆ—è¡¨ç±»å‹ï¼‰å±æ€§ä¸º: (256, 1, 28, 28)ï¼Œæ‰€ä»¥x.shape[0]=256ã€‚x.view(256, -1)æ˜¯ Tensor:(256, 784)
        y = self.sm(y)
        return y


net = softmax_net(input_num, output_num)  # input_num=784ï¼Œoutput_num=10

# åˆå§‹åŒ–æƒé‡å’Œåå·®
# åœ¨å“ªé‡Œè®¾ç½® requires_grad çš„å‚æ•°å‘¢
Init.normal_(net.linear.weight, mean=0, std=0.01)
Init.constant_(net.linear.bias, val=0)

# æŸå¤±å‡½æ•°
# nnæ¨¡å—å®ç°äº¤å‰ç†µæŸå¤±å‡½æ•°--åŒ…å«äº†softmaxå‡½æ•°
loss = nn.CrossEntropyLoss()

# å®šä¹‰ä¼˜åŒ–å‡½æ•°-å°æ‰¹é‡æ¢¯åº¦ä¸‹é™
optimizer = torch.optim.SGD(net.parameters(), lr=0.1)

# è¿­ä»£
epochs = 10

for epoch in range(epochs):
    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
    for X, y in train_iter:
        pre_y = net(X)
        l = loss(pre_y, y).sum()
        optimizer.zero_grad()  # å…ˆæ¢¯åº¦æ¸…é›¶
        l.backward()
        optimizer.step()
        # è®¡ç®—æ¯ä¸ªepochçš„loss
        train_l_sum += l.item()
        n += y.shape[0]
    print('epoch %d, loss %.4f, train acc %.3f' % (epoch + 1, train_l_sum / n, train_acc_sum / n))
