# åˆ©ç”¨torch.nnå®ç° softmax å›å½’åœ¨Fashion-MNISTæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œ
# å¹¶ä»lossï¼Œè®­ç»ƒé›†ä»¥åŠæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ç­‰å¤šä¸ªè§’åº¦å¯¹ç»“æœè¿›è¡Œåˆ†æ
# https://blog.csdn.net/qq_37534947/article/details/108179408


# æ³¨æ„çš„ç‚¹
# 1ã€lossè®°å¾— .sum()
# 2ã€å…ˆæ¢¯åº¦æ¸…é›¶

# é—®é¢˜ï¼š
# 1ã€å®šä¹‰æ¨¡å‹æ—¶ï¼šä¸ºå•¥è¿™é‡Œæ²¡æœ‰åƒ ä½œä¸š3ä¸€æ ·ï¼Œå…ˆè¿›è¡Œ çº¿æ€§åˆ¤åˆ« å†è¿›è¡Œ éçº¿æ€§å†³ç­–å‘¢ï¼Ÿåªçœ‹åˆ°äº†çº¿æ€§å‡½æ•°ï¼Œæ²¡æœ‰çœ‹åˆ°å†³ç­–å‡½æ•°ã€‚å›°æƒ‘

import torch
import torch.nn as nn
from torch.nn import init as Init  # è°ƒç”¨ğ¢ğ§ğ¢ğ­. ğ§ğ¨ğ«ğ¦ğšğ¥æ¨¡å—é€šè¿‡æ­£æ€åˆ†å¸ƒå¯¹çº¿æ€§ å›å½’ä¸­çš„æƒé‡å’Œåå·®è¿›è¡Œåˆå§‹åŒ–
import numpy as np
import torchvision
import torchvision.transforms as transforms

# ä¸‹è½½çš„æ•°æ®
mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=False,
                                                transform=transforms.ToTensor())

# è¯»å–æ•°æ®
batch_size = 256
train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)

# æ„å»ºæ¨¡å‹
input_num = 784
output_num = 10


# --------------------------------------æ³¨ï¼šè¿™é‡Œå’Œä¹‹å‰çš„logisticå®ç°å‡ ä¹ä¸€æ ·ï¼Œä¸»è¦æ˜¯å¯¹äºè¯¥æ¨¡å‹çš„è¾“å‡ºä»1å˜æˆ10.
class softmax_net(nn.Module):
    def __init__(self, input_num, output_num):
        super(softmax_net, self).__init__()
        self.linear = nn.Linear(input_num, output_num)

    def forward(self, x):  # x shape: (batch, 1, 28, 28)
        y = self.linear(x.view(x.shape[0], -1))
        return y


net = softmax_net(input_num, output_num)

# åˆå§‹åŒ–æƒé‡å’Œåå·®
Init.normal_(net.linear.weight, mean=0, std=0.01)
Init.constant_(net.linear.bias, val=0)

# æŸå¤±å‡½æ•°
# nnæ¨¡å—å®ç°äº¤å‰ç†µæŸå¤±å‡½æ•°--åŒ…å«äº†softmaxå‡½æ•°
loss = nn.CrossEntropyLoss()

# å®šä¹‰ä¼˜åŒ–å‡½æ•°-å°æ‰¹é‡æ¢¯åº¦ä¸‹é™
optimizer = torch.optim.SGD(net.parameters(), lr=0.1)

# è¿­ä»£
epochs = 10

for epoch in range(epochs):
    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
    for X, y in train_iter:
        pre_y = net(X)
        l = loss(pre_y, y).sum()
        optimizer.zero_grad()  # å…ˆæ¢¯åº¦æ¸…é›¶
        l.backward()
        optimizer.step()
        # è®¡ç®—æ¯ä¸ªepochçš„loss
        train_l_sum += l.item()
        n += y.shape[0]
    print('epoch %d, loss %.4f, train acc %.3f' % (epoch + 1, train_l_sum / n, train_acc_sum / n))
