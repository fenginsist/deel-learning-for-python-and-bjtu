# åˆ©ç”¨ torch.nn å®ç° logistic å›å½’åœ¨äººå·¥æ„é€ çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå¹¶å¯¹ç»“æœè¿›è¡Œåˆ†æï¼Œ å¹¶ä»lossä»¥åŠè®­ç»ƒé›†ä¸Šçš„å‡†ç¡®ç‡ç­‰å¤šä¸ªè§’åº¦å¯¹ç»“æœè¿›è¡Œåˆ†æ

# é—®é¢˜1ï¼šé€šè¿‡ torch å®ç°logistic æ—¶ï¼Œæƒé‡å’Œåå·®çš„åˆå§‹åŒ–ï¼Œæ—¶åˆå§‹åŒ– Liner å†…éƒ¨çš„ w b å—ï¼Ÿ
# é—®é¢˜2ï¼š

import torch
import torch.nn as nn
from torch.nn import init  # è°ƒç”¨ğ¢ğ§ğ¢ğ­. ğ§ğ¨ğ«ğ¦ğšğ¥æ¨¡å—é€šè¿‡æ­£æ€åˆ†å¸ƒå¯¹çº¿æ€§ å›å½’ä¸­çš„æƒé‡å’Œåå·®è¿›è¡Œåˆå§‹åŒ–
import torch.utils.data as Data

n_data = torch.ones(50, 2)  # æ•°æ®çš„åŸºæœ¬å½¢æ€
x1 = torch.normal(2 * n_data, 1)  # shape=(50, 2)
y1 = torch.zeros(50)  # ç±»å‹0 shape=(50)
x2 = torch.normal(-2 * n_data, 1)  # shape=(50, 2)
y2 = torch.ones(50)  # ç±»å‹1 shape=(50)

# æ³¨æ„ features, labels æ•°æ®çš„æ•°æ®å½¢å¼ä¸€å®šè¦åƒä¸‹é¢ä¸€æ · (torch.cat æ˜¯åˆå¹¶æ•°æ®)
features = torch.cat((x1, x2), 0).type(torch.FloatTensor)
labels = torch.cat((y1, y2), 0).type(torch.FloatTensor)

batch_size = 50
# å°†è®­ç»ƒæ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ç»„åˆ
dataset = Data.TensorDataset(features, labels)
# æŠŠ dataset æ”¾å…¥ DataLoader
data_iter = Data.DataLoader(
    dataset=dataset,  # torch TensorDataset format
    batch_size=batch_size,  # mini batch size
    shuffle=True,  # æ˜¯å¦æ‰“ä¹±æ•°æ® (è®­ç»ƒé›†ä¸€èˆ¬éœ€è¦è¿›è¡Œæ‰“ä¹±)
    num_workers=0,  # å¤šçº¿ç¨‹æ¥è¯»æ•°æ®ï¼Œ æ³¨æ„åœ¨Windowsä¸‹éœ€è¦è®¾ç½®ä¸º0
)


# æ„å»ºæ¨¡å‹
class LogisticRegression(nn.Module):
    def __init__(self, n_features):
        super(LogisticRegression, self).__init__()
        self.liner = nn.Linear(n_features, 1)   # åˆå§‹åŒ– çº¿æ€§åˆ¤åˆ«å‡½æ•°ï¼Œé‡Œé¢é»˜è®¤ä¼šæœ‰  æƒé‡å’Œåå·®
        self.sm = nn.Sigmoid()                  # åˆå§‹åŒ– éçº¿æ€§ å†³ç­–å‡½æ•°

    # å…ˆ æ‰§è¡Œ çº¿æ€§åˆ¤åˆ«å‡½æ•°ï¼Œå†æ‰§è¡Œ éçº¿æ€§å†³ç­–å‡½æ•°
    def forward(self, x):
        x = self.liner(x)
        x = self.sm(x)
        return x


num_inputs = 2
logistic_model = LogisticRegression(num_inputs)

# ç»™ æ¨¡å‹ä¸­ çº¿æ€§åˆ¤åˆ«å‡½æ•° è¿›è¡Œï¼Œåˆå§‹åŒ– æƒé‡å’Œ åå·®
init.normal_(logistic_model.liner.weight, mean=0, std=0.01)
init.constant_(logistic_model.liner.bias, val=0)               # ä¹Ÿå¯ä»¥ç›´æ¥ä¿®æ”¹biasçš„dataï¼š net[0].bias.data.fill_(0)
print(logistic_model.liner.weight)
print(logistic_model.liner.weight.size())   # torch.Size([1, 2])
print(logistic_model.liner.bias)
print(logistic_model.liner.bias.size())     # torch.Size([1])

# æŸå¤±å‡½æ•°ï¼šäºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°
loss = torch.nn.BCELoss()

# ä¼˜åŒ–å™¨
## ä¼ å…¥ æ¨¡å‹ ä¸­çš„ å‚æ•°ï¼Œä¹Ÿæ˜¯å†…åµŒçš„
optimizer = torch.optim.SGD(logistic_model.parameters(), lr=1e-3)

epoch = 10
# è¿­ä»£
# æ³¨ï¼šä¸åŒäºè‡ªå®šä¹‰çš„æ¨¡å‹ç­‰è®­ç»ƒè¿‡ç¨‹ï¼Œè¿™é‡Œçš„ æ¢¯åº¦æ¸…é›¶ä»¥åŠæ¢¯åº¦æ›´æ–° éƒ½æ˜¯åœ¨ä¼˜åŒ–å™¨çš„å‡½æ•°ä¸­è¿›è¡Œ
for i in range(epoch):
    train_l_num, train_acc_num, n = 0.0, 0.0, 0
    for X, y in data_iter:
        pred_y = logistic_model(X)
        l = loss(pred_y, y.view(-1, 1))
        optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶ï¼Œç­‰ä»·äºlogistic_model.zero_grad()
        l.backward()
        optimizer.step()
        train_l_num += l.item()  # è®¡ç®—æ¯ä¸ªepochçš„loss

        pred_y = torch.squeeze(torch.where(pred_y > 0.5, torch.tensor(1.0), torch.tensor(0.0)))  # è®¡ç®—è®­ç»ƒæ ·æœ¬çš„å‡†ç¡®ç‡
        train_acc_num += (pred_y == y).sum().item()

        n += y.shape[0]  # æ¯ä¸€ä¸ªepochçš„æ‰€æœ‰æ ·æœ¬æ•°
    print('epoch %d, loss %.4f,train_acc %f' % (i + 1, train_l_num / n, train_acc_num / n))
