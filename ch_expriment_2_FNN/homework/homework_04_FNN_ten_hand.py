# æ‰‹åŠ¨å®ç° FNN è§£å†³ å¤šåˆ†ç±»é—®é¢˜
# æ‰‹åŠ¨ä»¥åŠä½¿ç”¨torch.nnå®ç°å‰é¦ˆç¥ç»ç½‘ç»œå®éªŒï¼šhttps://blog.csdn.net/m0_52910424/article/details/127819278

import torch
import numpy as np
import random
import time
import torchvision
import torchvision.transforms as transforms

# RuntimeError: grad can be implicitly created only for scalar outputs
# è¯¥é”™è¯¯æ˜¯ï¼šåªèƒ½éšå¼åœ°ä¸ºæ ‡é‡è¾“å‡ºåˆ›å»ºGradã€‚
# è¯¥é”™è¯¯ç ”ç©¶äº†å¥½ä¹…ï¼Œå‘ç°æŸå¤±å‡½æ•°è¿”å›çš„æ˜¯å‘é‡ï¼Œåº”è¯¥æ˜¯æ ‡é‡ï¼Œæ‰€ä»¥åŠ ä¸Štorch.mean() å³å¯è§£å†³æ­¤é—®é¢˜ã€‚

# è®­ç»ƒé›†
train_dataset = torchvision.datasets.FashionMNIST(root="./Datasets/FashionMNIST/train", train=True,
                                                  transform=transforms.ToTensor(),
                                                  download=False)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)
# æµ‹è¯•é›†
test_dataset = torchvision.datasets.FashionMNIST(root="./Datasets/FashionMNIST/test", train=False,
                                                 transform=transforms.ToTensor())
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)


# 2&3)å®šä¹‰æ¨¡å‹åŠå…¶å‰å‘ä¼ æ’­è¿‡ç¨‹:ä¸€å±‚ç¥ç»ç½‘ç»œ ä¸€å±‚éšè—å±‚
# 2) å®šä¹‰æ¨¡å‹
class MyTenNet():
    def __init__(self):
        # è®¾ç½®è¾“å…¥å±‚ï¼ˆç‰¹å¾æ•°ï¼‰ã€éšè—å±‚ï¼ˆç¥ç»å…ƒä¸ªæ•°ï¼‰ã€è¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°
        num_inputs, num_hiddens, num_outputs = 28 * 28, 256, 10  # è¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å‡ºå±‚ çš„ ç¥ç»å…ƒä¸ªæ•°
        # ä»¥ä¸‹æ˜¯ä¸€å±‚ç¥ç»ç½‘ç»œï¼ˆéšè—å±‚ï¼‰çš„éšè—å±‚å’Œè¾“å‡ºå±‚çš„æƒé‡å’Œåå·®ã€‚
        w1 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_inputs)), dtype=torch.float, requires_grad=True)
        b1 = torch.zeros(num_hiddens, dtype=torch.float32, requires_grad=True)
        w2 = torch.tensor(np.random.normal(0, 0.01, (num_outputs, num_hiddens)), dtype=torch.float, requires_grad=True)
        b2 = torch.zeros(num_outputs, dtype=torch.float32, requires_grad=True)
        self.params = [w1, b1, w2, b2]

        # å®šä¹‰æ¨¡å‹ç»“æ„ *****: å®šä¹‰æ¿€æ´»å‡½æ•°ï¼Œè¾“å…¥å±‚å˜æˆä¸€ä¸ªä¸€ç»´å‘é‡ã€éšè—å±‚æ¿€æ´»å‡½æ•° relu å‡½æ•°ã€è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°
        self.input_layer = lambda x: x.view(x.shape[0], -1)  # æ¨¡å‹è¾“å…¥ï¼Œæ‹‰å¹³æˆå‘é‡
        self.hidden_layer = lambda x: self.my_ReLu(torch.matmul(x, w1.t()) + b1)  # éšè—å±‚ï¼Œä½¿ç”¨ relu å‡½æ•°ã€‚
        self.output_layer = lambda x: torch.matmul(x, w2.t()) + b2  # æ¨¡å‹è¾“å‡º

    def my_ReLu(self, x):
        return torch.max(input=x, other=torch.tensor(0.0))

    # 3) å®šä¹‰æ¨¡å‹å‰å‘ä¼ æ’­è¿‡ç¨‹
    # å¤šåˆ†ç±»ï¼Œä¸ºå•¥æ²¡æœ‰çœ‹åˆ°ä½¿ç”¨ softmax éçº¿æ€§å†³ç­–å‡½æ•°å‘¢ã€‚ç­”æ¡ˆåœ¨ä¸‹é¢
    def forward(self, x):
        flatten_input = self.input_layer(x)
        hidden_output = self.hidden_layer(flatten_input)
        final_output = self.output_layer(hidden_output)
        """
        å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåˆ†å¼€å®šä¹‰ ğ¬ğ¨ğŸğ­ğ¦ğšğ± è¿ç®—å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°å¯èƒ½ä¼šé€ æˆæ•°å€¼ä¸ç¨³å®šã€‚
        å› æ­¤ï¼ŒPyTorchæä¾›äº†ä¸€ä¸ªåŒ…æ‹¬ ğ¬ğ¨ğŸğ­ğ¦ğšğ± è¿ç®—å’Œäº¤å‰ç†µæŸå¤±è®¡ç®—çš„å‡½æ•°ã€‚å®ƒçš„æ•°å€¼ç¨³ å®šæ€§æ›´å¥½ã€‚
        æ‰€ä»¥åœ¨è¾“å‡ºå±‚ï¼Œæˆ‘ä»¬ä¸éœ€è¦å†è¿›è¡Œ ğ¬ğ¨ğŸğ­ğ¦ğšğ± æ“ä½œ
        """
        return final_output


# æŸå¤±å‡½æ•°:äº¤å‰ç†µæŸå¤±å‡½æ•°
def cross_entropy(y_hat, y):
    # return -torch.log(y_hat.gather(1, y.view(-1, 1)))
    return torch.mean(-torch.log(y_hat.gather(1, y.view(-1, 1))))


# ä¼˜åŒ–ç®—æ³•
def mySGD(params, lr, batch_size):
    for param in params:
        param.data -= lr * param.grad / batch_size


model = MyTenNet()
loss = cross_entropy  #
sgd = mySGD
lr = 0.15
batch_size = 128  # å’Œä¸Šé¢çš„æ•°æ®é›†çš„ batch ä¿æŒä¸€è‡´ã€‚
epochs = 40
train_all_loss = []  # è®°å½•è®­ç»ƒé›†ä¸Šå¾—losså˜åŒ–
test_all_loss = []  # è®°å½•æµ‹è¯•é›†ä¸Šçš„losså˜åŒ–
train_ACC, test_ACC = [], []  # è®°å½•æ­£ç¡®çš„ä¸ªæ•°
begin_time = time.time()
print('------------------å¼€å§‹è®­ç»ƒå•¦')
for epoch in range(epochs):
    train_l, train_acc_num = 0, 0
    for data, label in train_loader:
        pre = model.forward(data)
        l = loss(pre, label).sum()  # è®¡ç®—æ¯æ¬¡çš„æŸå¤±å€¼
        train_l += l.item()
        l.backward()  # åå‘ä¼ æ’­
        sgd(model.params, lr, batch_size)  # ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™è¿­ä»£æ¨¡å‹å‚æ•°
        # æ¢¯åº¦æ¸…é›¶
        train_acc_num += (pre.argmax(dim=1) == label).sum().item()
        for param in model.params:
            param.grad.data.zero_()

        # print(train_each_loss)
    train_all_loss.append(train_l)  # æ·»åŠ æŸå¤±å€¼åˆ°åˆ—è¡¨ä¸­
    train_ACC.append(train_acc_num / len(train_dataset))  # æ·»åŠ å‡†ç¡®ç‡åˆ°åˆ—è¡¨ä¸­
    # print('-----------------------å¼€å§‹æµ‹è¯•å•¦')
    with torch.no_grad():
        test_loss, test_acc_num = 0, 0
        for X, y in test_loader:
            p = model.forward(X)  # å»æ‰äº† .sum()
            ll = loss(p, y)
            test_loss += ll.item()
            test_acc_num += (p.argmax(dim=1) == y).sum().item()
        test_all_loss.append(test_loss)
        test_ACC.append(test_acc_num / len(test_dataset))  # # æ·»åŠ å‡†ç¡®ç‡åˆ°åˆ—è¡¨ä¸­
    if epoch == 0 or (epoch + 1) % 4 == 0:
        print('epoch: %d | train loss:%.5f | test loss:%.5f | train acc: %.2f | test acc: %.2f'
              % (epoch + 1, train_l, test_loss, train_ACC[-1], test_ACC[-1]))
end_time = time.time()
print("æ‰‹åŠ¨å®ç°å‰é¦ˆç½‘ç»œ-å¤šåˆ†ç±»å®éªŒ %dè½® æ€»ç”¨æ—¶: %.3fs" % (epochs, begin_time - end_time))
