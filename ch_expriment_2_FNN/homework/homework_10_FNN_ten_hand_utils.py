import torch
import torch.nn as nn
import numpy as np
import random
import time
import torchvision
import torchvision.transforms as transforms

"""
æ‰‹åŠ¨å®ç° FNN è§£å†³ å¤šåˆ†ç±»é—®é¢˜ çš„å·¥å…·ç±»ï¼Œä¸‹é¢çš„æ–‡ä»¶éƒ½åŸºäºæ­¤åšå®éªŒï¼Œè¿›è¡Œæµ‹è¯• momentumã€rmspropã€adam ä¸‰ç§ä¼˜åŒ–æ–¹æ³•
æ‰‹åŠ¨ä»¥åŠä½¿ç”¨torch.nnå®ç°å‰é¦ˆç¥ç»ç½‘ç»œå®éªŒï¼šhttps://blog.csdn.net/m0_52910424/article/details/127819278
"""


# è®­ç»ƒé›†
batch_size = 128
train_dataset = torchvision.datasets.FashionMNIST(root="./Datasets/FashionMNIST/train", train=True,
                                                  transform=transforms.ToTensor(),
                                                  download=False)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# æµ‹è¯•é›†
test_dataset = torchvision.datasets.FashionMNIST(root="./Datasets/FashionMNIST/test", train=False,
                                                 transform=transforms.ToTensor())
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)


# 2&3)å®šä¹‰æ¨¡å‹åŠå…¶å‰å‘ä¼ æ’­è¿‡ç¨‹:ä¸€å±‚ç¥ç»ç½‘ç»œ ä¸€å±‚éšè—å±‚
# 2) å®šä¹‰æ¨¡å‹
class MyTenNet():
    def __init__(self, dropout=0.0):
        # global dropout
        self.dropout = dropout              # å½“ dropout ä¸º0æ—¶ï¼Œå³ä»€ä¹ˆä¹Ÿéƒ½æ²¡æœ‰æ“ä½œ
        print('dropout: ', self.dropout)
        self.is_train = None

        # è®¾ç½®è¾“å…¥å±‚ï¼ˆç‰¹å¾æ•°ï¼‰ã€éšè—å±‚ï¼ˆç¥ç»å…ƒä¸ªæ•°ï¼‰ã€è¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°
        num_inputs, num_hiddens, num_outputs = 28 * 28, 256, 10  # è¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å‡ºå±‚ çš„ ç¥ç»å…ƒä¸ªæ•° # ååˆ†ç±»é—®é¢˜
        # ä»¥ä¸‹æ˜¯ä¸€å±‚ç¥ç»ç½‘ç»œï¼ˆéšè—å±‚ï¼‰çš„éšè—å±‚å’Œè¾“å‡ºå±‚çš„æƒé‡å’Œåå·®ã€‚
        w1 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_inputs)), dtype=torch.float, requires_grad=True)
        b1 = torch.zeros(num_hiddens, dtype=torch.float32, requires_grad=True)
        w2 = torch.tensor(np.random.normal(0, 0.01, (num_outputs, num_hiddens)), dtype=torch.float, requires_grad=True)
        b2 = torch.zeros(num_outputs, dtype=torch.float32, requires_grad=True)
        self.params = [w1, b1, w2, b2]

        # å®šä¹‰æ¨¡å‹ç»“æ„ *****: å®šä¹‰æ¿€æ´»å‡½æ•°ï¼Œè¾“å…¥å±‚å˜æˆä¸€ä¸ªä¸€ç»´å‘é‡ã€éšè—å±‚æ¿€æ´»å‡½æ•° relu å‡½æ•°ã€è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°
        self.input_layer = lambda x: x.view(x.shape[0], -1)  # æ¨¡å‹è¾“å…¥ï¼Œæ‹‰å¹³æˆå‘é‡
        self.hidden_layer = lambda x: self.my_relu(torch.matmul(x, w1.t()) + b1)  # éšè—å±‚ï¼Œä½¿ç”¨ relu å‡½æ•°ã€‚
        self.output_layer = lambda x: torch.matmul(x, w2.t()) + b2  # æ¨¡å‹è¾“å‡º

    def my_relu(self, x):
        return torch.max(input=x, other=torch.tensor(0.0))

    # ä»¥ä¸‹ä¸¤ä¸ªå‡½æ•°åˆ†åˆ«åœ¨è®­ç»ƒå’Œæµ‹è¯•å‰è°ƒç”¨ï¼Œé€‰æ‹©æ˜¯å¦éœ€è¦dropout
    def train(self):
        self.is_train = True

    def test(self):
        self.is_train = False

    """
        å®šä¹‰dropoutå±‚
        x: è¾“å…¥æ•°æ®
        dropout: éšæœºä¸¢å¼ƒçš„æ¦‚ç‡
        """

    def dropout_layer(self, x):
        dropout = self.dropout
        assert 0 <= dropout <= 1  # dropoutå€¼å¿…é¡»åœ¨0-1ä¹‹é—´
        # dropout==1ï¼Œæ‰€æœ‰å…ƒç´ éƒ½è¢«ä¸¢å¼ƒã€‚
        if dropout == 1:
            return torch.zeros_like(x)
            # åœ¨æœ¬æƒ…å†µä¸­ï¼Œæ‰€æœ‰å…ƒç´ éƒ½è¢«ä¿ç•™ã€‚
        if dropout == 0:
            return x
        mask = (torch.rand(x.shape) < 1.0 - dropout).float()  # rand()è¿”å›ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«äº†ä»åŒºé—´[0, 1)çš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–çš„ä¸€ç»„éšæœºæ•°
        return mask * x / (1.0 - dropout)

    # 3) å®šä¹‰æ¨¡å‹å‰å‘ä¼ æ’­è¿‡ç¨‹
    # å¤šåˆ†ç±»ï¼Œä¸ºå•¥æ²¡æœ‰çœ‹åˆ°ä½¿ç”¨ softmax éçº¿æ€§å†³ç­–å‡½æ•°å‘¢ã€‚ç­”æ¡ˆåœ¨ä¸‹é¢
    def forward(self, x):
        flatten_input = self.input_layer(x)
        if self.is_train:  # å¦‚æœæ˜¯è®­ç»ƒè¿‡ç¨‹ï¼Œåˆ™éœ€è¦å¼€å¯dropout å¦åˆ™ éœ€è¦å…³é—­ dropout
            flatten_input = self.dropout_layer(flatten_input)
        hidden_output = self.hidden_layer(flatten_input)
        if self.is_train:
            hidden_output = self.dropout_layer(hidden_output)
        final_output = self.output_layer(hidden_output)
        """
        å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåˆ†å¼€å®šä¹‰ ğ¬ğ¨ğŸğ­ğ¦ğšğ± è¿ç®—å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°å¯èƒ½ä¼šé€ æˆæ•°å€¼ä¸ç¨³å®šã€‚
        å› æ­¤ï¼ŒPyTorchæä¾›äº†ä¸€ä¸ªåŒ…æ‹¬ ğ¬ğ¨ğŸğ­ğ¦ğšğ± è¿ç®—å’Œäº¤å‰ç†µæŸå¤±è®¡ç®—çš„å‡½æ•°ã€‚å®ƒçš„æ•°å€¼ç¨³ å®šæ€§æ›´å¥½ã€‚
        æ‰€ä»¥åœ¨è¾“å‡ºå±‚ï¼Œæˆ‘ä»¬ä¸éœ€è¦å†è¿›è¡Œ ğ¬ğ¨ğŸğ­ğ¦ğšğ± æ“ä½œ
        """
        return final_output


# æŸå¤±å‡½æ•°:äº¤å‰ç†µæŸå¤±å‡½æ•°
def cross_entropy(y_hat, y):
    # return -torch.log(y_hat.gather(1, y.view(-1, 1)))
    return torch.mean(-torch.log(y_hat.gather(1, y.view(-1, 1))))


# å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ ä¼˜åŒ–ç®—æ³•
def mySGD(params, lr, batch_size):
    for param in params:
        param.data -= lr * param.grad / batch_size


# å®šä¹‰L2èŒƒæ•°æƒ©ç½šé¡¹ å‚æ•° w ä¸ºæ¨¡å‹çš„ w åœ¨æœ¬æ¬¡å®éªŒä¸­ä¸º[w_1, w_2] batch_size=128
def l2_penalty(w):
    cost = 0
    for i in range(len(w)):
        cost += (w[i] ** 2).sum()
    return cost / batch_size / 2


"""
å®šä¹‰è®­ç»ƒå‡½æ•°
model:å®šä¹‰çš„æ¨¡å‹ é»˜è®¤ä¸ºMyNet(0) å³æ— dropoutçš„åˆå§‹ç½‘ç»œ

init_statesï¼šæ˜¯ optimizer ä¼˜åŒ–å‡½æ•°çš„åˆå§‹åŒ–
optimizer:å®šä¹‰çš„ä¼˜åŒ–å‡½æ•°ï¼Œé»˜è®¤ä¸ºè‡ªå·±å®šä¹‰çš„mySGDå‡½æ•°

epochs:è®­ç»ƒæ€»è½®æ•° é»˜è®¤ä¸º30
lr :å­¦ä¹ ç‡ é»˜è®¤ä¸º0.1

L2ï¼šæ˜¯å¦éœ€è¦ L2æ­£åˆ™åŒ– 
lambdï¼šL2æ­£åˆ™åŒ–çš„ æƒ©ç½šç³»æ•°
"""


def train_and_test(model=MyTenNet(), init_states=None, optimizer=mySGD, epochs=20, lr=0.01, L2=False, lambd=0):
    train_all_loss = []  # è®°å½•è®­ç»ƒé›†ä¸Šå¾—losså˜åŒ–
    test_all_loss = []  # è®°å½•æµ‹è¯•é›†ä¸Šçš„losså˜åŒ–
    train_ACC, test_ACC = [], []  # è®°å½•æ­£ç¡®çš„ä¸ªæ•°
    begin_time = time.time()
    # æ¿€æ´»å‡½æ•°ä¸ºè‡ªå·±å®šä¹‰çš„mySGDå‡½æ•°
    # criterion = cross_entropy # æŸå¤±å‡½æ•°ä¸ºäº¤å‰ç†µå‡½æ•°
    loss = nn.CrossEntropyLoss()  # æŸå¤±å‡½æ•°

    """è¡¨æ˜å½“å‰å¤„äºè®­ç»ƒçŠ¶æ€ï¼Œå…è®¸ä½¿ç”¨dropout"""
    model.train()

    for epoch in range(epochs):
        train_l, train_acc_num = 0, 0
        for data, label in train_loader:
            pre = model.forward(data)
            l = loss(pre, label).sum()  # è®¡ç®—æ¯æ¬¡çš„æŸå¤±å€¼
            # è‹¥L2ä¸ºTrueåˆ™è¡¨ç¤ºéœ€è¦æ·»åŠ L2èŒƒæ•°æƒ©ç½šé¡¹
            if L2 == True:
                l += lambd * l2_penalty(model.w)

            train_l += l.item()
            l.backward()  # åå‘ä¼ æ’­
            # è‹¥å½“å‰statesä¸º Noneè¡¨ç¤º ä½¿ç”¨çš„æ˜¯ é»˜è®¤çš„ä¼˜åŒ–å‡½æ•°mySGD
            if init_states == None:
                optimizer(model.params, lr, 128)  # ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™è¿­ä»£æ¨¡å‹å‚æ•°
            # å¦åˆ™çš„è¯ä½¿ç”¨çš„æ˜¯è‡ªå·±å®šä¹‰çš„ä¼˜åŒ–å™¨ï¼Œé€šè¿‡ä¼ å…¥çš„å‚æ•°ï¼Œæ¥å®ç°ä¼˜åŒ–æ•ˆæœ
            else:
                states = init_states(model.params)
                optimizer(model.params, states, lr=lr)
            # æ¢¯åº¦æ¸…é›¶
            train_acc_num += (pre.argmax(dim=1) == label).sum().item()
            for param in model.params:
                param.grad.data.zero_()

            # print(train_each_loss)
        train_all_loss.append(train_l)  # æ·»åŠ æŸå¤±å€¼åˆ°åˆ—è¡¨ä¸­
        train_ACC.append(train_acc_num / len(train_dataset))  # æ·»åŠ å‡†ç¡®ç‡åˆ°åˆ—è¡¨ä¸­
        # print('-----------------------å¼€å§‹æµ‹è¯•å•¦')
        with torch.no_grad():
            is_train = False  # è¡¨æ˜å½“å‰ä¸ºæµ‹è¯•é˜¶æ®µï¼Œä¸éœ€è¦dropoutå‚ä¸
            test_loss, test_acc_num = 0, 0
            for X, y in test_loader:
                p = model.forward(X)  # å»æ‰äº† .sum()
                ll = loss(p, y)
                test_loss += ll.item()
                test_acc_num += (p.argmax(dim=1) == y).sum().item()
            test_all_loss.append(test_loss)
            test_ACC.append(test_acc_num / len(test_dataset))  # # æ·»åŠ å‡†ç¡®ç‡åˆ°åˆ—è¡¨ä¸­
        if epoch == 0 or (epoch + 1) % 4 == 0:
            print('epoch: %d | train loss:%.5f | test loss:%.5f | train acc: %.2f | test acc: %.2f'
                  % (epoch + 1, train_l, test_loss, train_ACC[-1], test_ACC[-1]))
    end_time = time.time()
    print("homework_10_FNN_ten_hand_utils.py:æ‰‹åŠ¨å®ç°å‰é¦ˆç½‘ç»œ-å¤šåˆ†ç±»å®éªŒ %dè½® æ€»ç”¨æ—¶: %.3fs" % (epochs, end_time - begin_time))
    return train_all_loss, test_all_loss, train_ACC, test_ACC