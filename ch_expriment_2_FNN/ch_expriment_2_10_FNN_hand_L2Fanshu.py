# å®éªŒ2ä¸­ åº”å¯¹è¿‡æ‹Ÿåˆé—®é¢˜çš„å¸¸ç”¨æ–¹æ³•â€”â€” L2èŒƒæ•°æ­£åˆ™åŒ–
# æ‰‹åŠ¨å®ç° L2èŒƒæ•°æ­£åˆ™åŒ–

import torch
import torch.nn as nn
from torch.utils import data
import numpy as np
import sys

sys.path.append("..")
from matplotlib import pyplot as plt
from IPython import display

import matplotlib_inline

n_train, n_test, num_inputs = 20, 100, 200
true_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05

# 1 ç”Ÿæˆæ•°æ®é›†
features = torch.randn((n_train + n_test, num_inputs))
labels = torch.matmul(features, true_w) + true_b
labels = torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float, requires_grad=True)
train_features, test_features = features[:n_train, :], features[n_train:, :]
train_labels, test_labels = labels[:n_train], labels[n_train:]
print(train_features[0][:5])  # è¾“å‡ºç¬¬ä¸€ä¸ªæ ·æœ¬ç‰¹å¾å‘é‡çš„ å‰äº”ç»´çš„å…ƒç´ 
print(train_labels[0])


# 1 å®šä¹‰æ¨¡å‹
def linear(X, w, b):
    return torch.mm(X, w) + b


# 2 æŸå¤±å‡½æ•°ï¼šå®šä¹‰å‡æ–¹è¯¯å·®
def squared_loss(y_hat, y):
    # è¿”å›çš„æ˜¯å‘é‡ï¼Œæ³¨æ„: pytorché‡Œçš„MSELoss å¹¶æ²¡æœ‰é™¤ä»¥2
    return ((y_hat - y.view(y_hat.size())) ** 2) / 2


# 3 æŸå¤±å‡½æ•°åä½¿ç”¨ï¼š æ‰‹åŠ¨å®ç°ğ‘³ğŸèŒƒæ•°æ­£åˆ™åŒ–
# 3.1 å®šä¹‰éšæœºåˆå§‹åŒ–æ¨¡å‹å‚æ•°çš„å‡½æ•°
def init_params():
    w = torch.randn((num_inputs, 1), requires_grad=True)  # num_inputs=200
    b = torch.zeros(1, requires_grad=True)
    return [w, b]


# 3.2 å®šä¹‰ ğ¿2èŒƒæ•°æƒ©ç½šé¡¹ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªfloatçš„æ•°
def penalty_12(w):
    return (w ** 2).sum() / 2


# 4 ä¼˜åŒ–å‡½æ•° å®šä¹‰éšæœºæ¢¯åº¦ä¸‹é™å‡½æ•°
def SGD(params, lr):
    for param in params:
        # æ³¨æ„è¿™é‡Œå‚æ•°èµ‹å€¼ç”¨çš„æ˜¯ param.data
        param.data -= lr * param.grad


def Draw_Loss_Curve(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None, legend=None, figsize=(3.5, 2.5)):
    # display.set_matplotlib_formats('svg')
    # ä¸Šé¢çš„æŠ¥é”™ï¼Œæ–¹æ³•è¢«èˆå¼ƒäº†ï¼Œç½‘ä¸Šè¯´ç”¨ä¸‹é¢è¿™ä¸ªåŒ…å’Œæ–¹æ³•
    matplotlib_inline.backend_inline.set_matplotlib_formats('svg')
    plt.rcParams['figure.figsize'] = figsize
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.semilogy(x_vals, y_vals)
    if x2_vals and y2_vals:
        plt.semilogy(x2_vals, y2_vals, linestyle=':')
        plt.legend(legend)


batch_size, num_opochs, lr = 1, 100, 0.003
net, loss = linear, squared_loss

# åˆ’åˆ†æ•°æ®é›†
dataset = torch.utils.data.TensorDataset(train_features, train_labels)
train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)


# è®­ç»ƒæ¨¡å‹
def fit_and_plot(lambd):
    w, b = init_params()
    train_ls, test_ls = [], []
    train_l_sum = 0
    for epoch in range(num_opochs):
        for X, y in train_iter:
            pre_y = net(X, w, b)
            # æ·»åŠ äº† L2 èŒƒæ•°æƒ©ç½šé¡¹
            l = (loss(pre_y, y) + lambd * penalty_12(w)).sum()

            if w.grad is not None:
                w.grad.data.zero_()
                b.grad.data.zero_()
            l.backward()
            SGD([w, b], lr)

            train_l_sum += l.item()
        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())
        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())
        print('epoch %d, train loss %.4f' % (epoch + 1, train_l_sum))
    Draw_Loss_Curve(range(1, num_opochs + 1), train_ls, 'epochs', 'loss', range(1, num_opochs + 1), test_ls, ['train', 'test'])
    print('L2 norm of w:', w.norm().item())


# ğœ† = 0(å³ä¸ä½¿ç”¨ğ¿"èŒƒæ•°æ­£åˆ™åŒ–)æ—¶çš„å®éªŒ ç»“æœï¼Œå‡ºç°äº†è¿‡æ‹Ÿåˆçš„ç°è±¡ã€‚
# fit_and_plot(0)
# ğœ† = 3(å³ä½¿ç”¨ğ¿"èŒƒæ•°æ­£åˆ™åŒ–)æ—¶çš„å®éªŒç»“æœ ï¼Œä¸€å®šç¨‹åº¦çš„ç¼“è§£äº†è¿‡æ‹Ÿåˆã€‚åŒæ—¶å¯ä»¥çœ‹åˆ° å‚æ•°ğ¿"èŒƒæ•°å˜å°ï¼Œå‚æ•°æ›´æ¥è¿‘0ã€‚
fit_and_plot(3)
